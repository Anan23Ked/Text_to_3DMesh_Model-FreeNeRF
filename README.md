This project aims to develop a novel way to generate 3D images from a 2D image by applying a pertained diffusion model to generate 2D images from a textual input and using this output to feed into a Frequency Regularised Neural Network that can generate accurate outputs even from sparse input and represent scenes as a Neural Radiance Fields for View Synthesis.

The algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (θ, φ)) and whose output is the volume density and view-dependent emitted radiance at that spatial location.
The views are synthesized by querying 5D coordinates along camera rays and using classic volume rendering techniques to project the output colours and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis.

For a more detailed description, check the Colab Notebook - https://colab.research.google.com/drive/1BVb6RX8XBTYtIwlXlOhbNkemXl57epvH?authuser=1#scrollTo=ips093qx2G0n
